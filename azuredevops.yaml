trigger:
  - master

variables:
  - name: AZURE_LOCATION
    value: australiasoutheast
  - group: workspace-enterprise-data-app-dev
  - group: enterprise-data-dev-workspace-keys

stages:
  - stage: build
    displayName: 'Build and test the website'
    jobs:
      - job: run_build
        pool:
          vmImage: 'ubuntu-latest'
        variables:
          - name: NotebookName
            value: helloworld
          - name: buildDir
            value: "$(Build.ArtifactStagingDirectory)"
        steps: 
          - bash: |
              mkdir -p "$(buildDir)/notebooks"
              find . -name '*.scala' | cpio -pdm $(buildDir)/notebooks
            displayName: 'Prepare Notebook Build Artifacts'
          - bash: |
              mkdir -p "$(buildDir)/adf_template"
              cp -R datafactory/* "$(buildDir)/adf_template/"
            displayName: 'Include Azure DataFactory Resource Manager templates into Build Artifacts'
          - task: PublishBuildArtifacts@1
            displayName: Publish DataFactory ARM Template Build Artifacts
            inputs:
              pathtoPublish: '$(buildDir)/adf_template'
              artifactName: adf_template
          - task: PublishBuildArtifacts@1
            displayName: Publish Notebook Build Artifacts
            inputs:
              pathtoPublish: '$(buildDir)/notebooks/'
              artifactName: notebooks

  # - stage: deploy
  #   displayName: 'Deploy Databricks Notebooks'
  #   jobs:
  #     - job: run_deploy
  #       pool:
  #         vmImage: 'ubuntu-latest'
  #       steps:
  #         - checkout: none
  #         - download: current
  #           artifact: notebooks
  #         - task: UsePythonVersion@0
  #           inputs:
  #             versionSpec: '3.7'
  #           displayName: 'Use Python 3.7'

  #         - bash: |
  #             python -m pip install --upgrade pip setuptools wheel databricks-cli
  #           displayName: 'Install Azure Databricks CLI'

  #         - bash: |
  #             databricks configure --token <<EOF
  #             https://$(AZURE_LOCATION).azuredatabricks.net
  #             $(DB_TOKEN)
  #             EOF
  #           displayName: 'Authenticate with Azure Databricks CLI'

  #         - bash: |
  #             databricks workspace mkdirs /Shared/pa_test_automation
  #             databricks workspace import_dir -o $(Pipeline.Workspace)/notebooks /Shared/pa_test_automation
  #           displayName: 'Upload notebooks to Azure Databricks'

  - stage: deploy_datafactory
    displayName: 'Deploy Azure DataFactory'
    jobs:
      - job: run_deploy_datafactory
        pool:
          vmImage: 'ubuntu-latest'
        steps:
          - checkout: none
          - download: current
            artifact: adf_template
          - bash: |
              sudo apt-get install -y azure-cli
              az login --service-principal --u "$(workspace_principal_id)" --p "$(workspace_principal_secret)" --tenant $(TENANT_ID)
            displayName: 'Login with Azure CLI'
          - bash: az account set --subscription $(SUBSCRIPTION_ID)
            displayName: 'Setup Subscription'
          - bash: az group deployment create --name $(Build.SourceVersion) --resource-group $(RESOURCE_GROUP) --mode Incremental --template-file $(Pipeline.Workspace)/adf_template/arm_template.json --parameters $(Pipeline.Workspace)/adf_template/arm_template_parameters.json --parameters factoryName=infomart-sandbox-00
            displayName: 'Uploading ARM template'