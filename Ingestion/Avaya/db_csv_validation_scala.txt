//libraries
import scala.util.control._
import java.util.Properties
import java.text.SimpleDateFormat
import java.util.Calendar
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col,lower,when,length,concat,lit,trim}
import org.apache.spark.sql.{DataFrame,Column}
import org.apache.spark.sql.types.DataTypes
import scala.collection.immutable.List
import java.time.LocalDateTime;

val startTimeMillis = System.currentTimeMillis()

//This function will replace empty string with null string in all columns in a dataframe
def replaceEmptyStringsWithNull_allColumns(df: DataFrame): DataFrame = {
  df.schema.foldLeft(df)(
    (current, field) =>
      field.dataType match {
        case DataTypes.StringType =>
          current.withColumn(
            field.name,
            when(length(col(field.name)) === 0, lit(null: String)).otherwise(col(field.name))
          )
        case _ => current
      }
  )
} 

//This function will trim a single column in a dataframe
def trimColumn(df: DataFrame, colName: String): DataFrame = {    
  var newDf = df.withColumn(colName, trim(col(colName)))
  return newDf
}

//This function will trim all columns in a dataframe
def trimAllColumns(df: DataFrame): DataFrame = {  
  var newDf = df.columns.foldLeft(df) { (memoDF, colName) =>
                      memoDF.withColumn(
                        colName,      
                        trim(col(colName))
                      )
  }  
  return newDf
}

//This function will replace empty string with null string in a single column in a dataframe
def replaceEmptyStringsWithNull_perColumn(df: DataFrame, colName: String): DataFrame = {    
  var newDf = df.withColumn(colName, when(length(col(colName)) === 0, lit(null: String)).otherwise(col(colName)))
  return newDf
}

//This function will drop a column from a dataframe
def dropColumn(df: DataFrame, colName: String): DataFrame = {    
  var newDf: DataFrame = null
  newDf = df.drop(colName)  
  return newDf
}

//This function will take dataframe that needs to be cleaned and a dataframe of known issues
//It will clean the dataframe as per the known issues and return a cleaned dataframe
def fixKnownDataIssues(tobecleaned_df: DataFrame, kwnissues_orgn_colnme_func_df: DataFrame): DataFrame = { 
  var column_list: List[String] = kwnissues_orgn_colnme_func_df.select("column_name").map(_.getString(0)).collect().toList
  var function_names = kwnissues_orgn_colnme_func_df.select("function").collect().map(_.getString(0)).toList
  var newDf: DataFrame = null
  var tempDf: DataFrame = tobecleaned_df
  
  for(counter <- 0 until column_list.size){    
    var column_name_arry = column_list(counter).split(";")
    var function_name = function_names(counter)
    
    for(column_name <- column_name_arry) {      
      newDf = determineActionAndExecute(tempDf, column_name, function_name)
      tempDf = newDf
    }      
  }
  newDf = tempDf
  return newDf
}


//This function will take the dataframe that needs to be cleaned, column name that needs cleaning and function to be performed on the column
def determineActionAndExecute(tobecleaned_df: DataFrame, column_name: String, func_name: String): DataFrame = { 
  var newDf: DataFrame = null
  
  if(func_name.toLowerCase().trim().equals("drop_columns")){
    newDf = dropColumn(tobecleaned_df, column_name)
  }else if(func_name.toLowerCase().trim().equals("trim_columns")){
    newDf = trimColumn(tobecleaned_df, column_name)
  }else if(func_name.toLowerCase().trim().equals("replace_emptystring_with_null")){
    newDf = replaceEmptyStringsWithNull_perColumn(tobecleaned_df, column_name)
  }  
  return newDf
}

 
//DB Connection String
val jdbcHostname = "10.216.2.181"
val jdbcPort = 1433
val jdbcDatabase = "CentralDWH_LS"  
val jdbcUrl = s"jdbc:sqlserver://10.216.2.181:1433;database=CentralDWH_LS;"
val connectionProperties = new Properties()
val driverClass = "com.microsoft.sqlserver.jdbc.SQLServerDriver"


connectionProperties.put("user", s"CDLread")
connectionProperties.put("password", dbutils.secrets.get(scope = "adbtstkv00", key = "avayadb"))    
connectionProperties.setProperty("Driver", driverClass)      


// CONNECTING TO THE LANDING ZONE
dbutils.fs.refreshMounts()
spark.conf.set("fs.azure.account.key.edplztstsa00.blob.core.windows.net", dbutils.secrets.get(scope = "adbtstkv00", key = "edplztstsa00key"))
val strUsername ="edplztstsa00"       // Source blob location(CSV and JSON)
val lz_blobName = "others"
val sourcefolder = "avaya"

// *****LOADING TABLE LIST*****
var parenttablelistpath = "wasbs://test-parameters@edplztstsa00.blob.core.windows.net/table_list/db_csv/ingestion_avaya/DB_CSV_TestData.csv"
var parentTable_list = spark.read.format("csv").option("header", "true").load(parenttablelistpath).toDF
parentTable_list.createOrReplaceTempView("parentTable_list")
var tableNames = parentTable_list.select("tablename").filter("sourcefolder='"+ sourcefolder +"'").collect().map(_(0)).toList
var extractDates = parentTable_list.select("extract_date").filter("sourcefolder='"+ sourcefolder +"'").collect().map(_(0)).toList

// *****LOADING KNOWN ISSUES LIST*****
var knownissueslistpath = "wasbs://test-parameters@edplztstsa00.blob.core.windows.net/table_list/db_csv/ingestion_avaya/known_data_issues.csv"
var knownissues_list = spark.read.format("csv").option("header", "true").load(knownissueslistpath).toDF
knownissues_list.createOrReplaceTempView("knownissues_list")



for (iterator <- 0 until tableNames.length){
  
     var srcTableName = tableNames(iterator)
     var extractDate = extractDates(iterator)
    
     var startTimeForTableMillis = System.currentTimeMillis()
  
    // Read Avaya data into dataframe    
     var df_srcTable = spark.read.jdbc(jdbcUrl, "dbo." + srcTableName, connectionProperties)
     df_srcTable.createOrReplaceTempView("df_srcTable")

    //Read CSV into dataframe
     var blob_folderName = "wasbs://" + lz_blobName + "@" + strUsername + ".blob.core.windows.net/" + sourcefolder + "/" + srcTableName
     var blob_folderName_extractPath = blob_folderName + "/1/extract_date=" + extractDate
     var csvFileNames = dbutils.fs.ls(blob_folderName_extractPath)     
            
     println("\n")
     println("******************START TESTCASE EXECUTION*************************")
     println("Database table name: " + srcTableName)
     println("Extract Date: " + extractDate)
     println("CSV File names: ")      
     csvFileNames.map(_.path).filter(_.contains("csv.bz2")).foreach(path => println("" + path))
     println("Test case execution date: " + LocalDateTime.now())     
     println("\n")
     
     var df_csv = spark.read.format("csv").option("header", "false").option("delimiter", "\u0001").load(blob_folderName_extractPath + "/*.csv.bz2")
     df_csv.createOrReplaceTempView("df_csv")
  
     //Code to validate counts between DB table and CSV
     println("Execute count comparison between Database table and CSV") 
     var srcTable_count = df_srcTable.count() 
     var csv_count = df_csv.count()
     println("Database table Count is : " + srcTable_count)
     println("CSV Count is : " + csv_count)
     

     if ((srcTable_count - csv_count) == 0){
         println("Data count matching for both Database table and CSV --- PASS.")
     } else{
         println("Data count not matching for both Database table and CSV --- FAIL.")
     }
     println("*******************************************")
     println("\n")
     
     println("Execute data comparison between Database table and CSV ")
    //Code to validate data between DB table and CSV  
     var kwnissues_orgn_colnme_func_df = knownissues_list.select("table_name","dataorigin","column_name","function","issuearea").filter("functionality='avaya_ingestion'").filter("table_name='" + srcTableName + "'")
     var db_kwnissues_orgn_colnme_func_df = kwnissues_orgn_colnme_func_df.filter("dataorigin='database'")
     var csv_kwnissues_orgn_colnme_func_df = kwnissues_orgn_colnme_func_df.filter("dataorigin='csv'")
  
     db_kwnissues_orgn_colnme_func_df.createOrReplaceTempView("db_kwnissues_orgn_colnme_func_df")
     csv_kwnissues_orgn_colnme_func_df.createOrReplaceTempView("csv_kwnissues_orgn_colnme_func_df")

    //Clean dataframe as per known issues      
     var df_srcTable_temp: DataFrame = fixKnownDataIssues(df_srcTable,db_kwnissues_orgn_colnme_func_df)
     var df_csv_temp: DataFrame = fixKnownDataIssues(df_csv,csv_kwnissues_orgn_colnme_func_df)     
     
   
     df_srcTable_temp.createOrReplaceTempView("df_srcTable_temp")
     df_csv_temp.createOrReplaceTempView("df_csv_temp")
     
     df_srcTable = df_srcTable_temp   
     df_csv = df_csv_temp
     
     println("Following transformations were applied on the DB table dataframe to handle known issues: ")
     db_kwnissues_orgn_colnme_func_df.show(db_kwnissues_orgn_colnme_func_df.count().toInt, false)
    
     println("Following transformations were applied on the CSV dataframe to handle known issues: ")
     csv_kwnissues_orgn_colnme_func_df.show(csv_kwnissues_orgn_colnme_func_df.count().toInt, false)
  
    println("Dataframe comparison between source and target")
  
     var df_resultSrcTableMinCsv = df_srcTable.except(df_csv) 
     var df_resultCsvMinSrcTable = df_csv.except(df_srcTable)    
     df_resultSrcTableMinCsv.createOrReplaceTempView("df_resultSrcTableMinCsv")
     df_resultCsvMinSrcTable.createOrReplaceTempView("df_resultCsvMinSrcTable")
     var srcTable_minus_csv_count = df_resultSrcTableMinCsv.count() 
     var csv_minus_srcTable_count = df_resultCsvMinSrcTable.count()
     
  
     println("Database table minus CSV comparison output using scala except function: ")
     spark.sql("SELECT * FROM df_resultSrcTableMinCsv" ).show()
     println("Database table minus CSV mismatch count : " + srcTable_minus_csv_count)

     println("CSV VS Database table comparison output using scala except function:")
     spark.sql("SELECT * FROM df_resultCsvMinSrcTable" ).show()
     println("CSV minus Database table mismatch count : " + csv_minus_srcTable_count)    
     
    if((srcTable_minus_csv_count != 0) || (csv_minus_srcTable_count != 0)){
      println("Database data not matched with CSV data - FAIL")
      println("Top 10 Sequence number not matching: ")
      df_resultCsvMinSrcTable.select($"_c0").limit(10).show(5,false)
      df_resultSrcTableMinCsv.select($"sid_key").limit(10).show(5,false)
    }else{
      println("Database data matched with CSV data successfully - PASS")
    }     
     println("\n\n")
     var endTimeForTableMillis = System.currentTimeMillis()    
     println("Total time in minutes for current test case is: " + ((endTimeForTableMillis-startTimeForTableMillis)/(1000 * 60)).toFloat)
     println("\n******************END TESTCASE EXECUTION*************************\n")
}
val endTimeMillis = System.currentTimeMillis()
println("Total time in minutes for entire test suite: " + ((endTimeMillis-startTimeMillis)/(1000 * 60)).toFloat)
println("\n#################################################################################\n")  
println("Note: Refer GIT for automation code used in above testing.")